%-------------------------------------------------------
%-- PREAMBLE
%-------------------------------------------------------
\documentclass[a4paper,10pt]{article} % Uses article class in A4 format

% Includes preamble configurations
\input{theme/assignment_preamble.tex}

% --------------------------------------------------------------------
% *************************** FRONT MATTER ***************************
% --------------------------------------------------------------------
\def\assnumber{12}
\def\title{Tensor Train Singular Value Decomposition (TTSVD)}
\def\author{OtacÃ­lio B. L. Neto}
\def\authorid{481736}
\def\email{minhotmog@alu.ufc.br}
\def\institute{Universidade Federal do CearÃ¡}
\def\course{TIP8419 - Tensor Algebra}
\def\professor{Prof. AndrÃ© A., Prof. Maryam D.} 

% ====================================================================

% --------------------------------------------------------------------
% ***************************** DOCUMENT *****************************
% --------------------------------------------------------------------
\begin{document} \sloppy

%-------------------------------------------------------
% TITLE PAGE
%-------------------------------------------------------
\thetitle

%-------------------------------------------------------
% PROBLEM 1
%-------------------------------------------------------
\vskip-0.4cm
\begin{problem} \justifying
We will go to implement the TT-SVD (Tensor Train Singular Value Decomposition) algorithm. The TT Decomposition of a $N$-order tensor $\mathcal{X} \in \mathbb{R}^{I_1\times I_2 \times \cdots \times I_N}$, with TT-Ranks ($R_1, R_2, \cdots, R_{N-1}$), is a decomposition of the tensor $\mathcal{X}$ into a train of tensors as 
\begin{equation*}
    \mathcal{X} = \mathbf{G}_1 \times_2^1 \mathcal{G}_2 \times_3^1 \mathcal{G}_3  \times_4^1 \cdots \times_{N-1}^1 \mathcal{G}_{N-1} \times_{N}^1 \mathbf{G}_N.
\end{equation*}

\noindent For a $4$-order tensor $\mathcal{X} \in \mathbb{R}^{5\times 5\times 5\times 5}$, implement the TT-SVD algorithm to estimate the TT-cores $\mathbf{G}_1 \in \mathbb{C}^{5\times R_1}$, $\mathcal{G}_2 \in \mathbb{C}^{R_{1}\times 5\times R_2}$, $\mathcal{G}_3 \in \mathbb{C}^{R_{2}\times 5\times R_3}$, and $\mathbf{G}_4 \in \mathbb{C}^{R_3 \times 5}$ that forms the TT Decomposition of $\mathcal{X}$, for given ranks ($R_1,R_2,R_3$). For randomly generated ($\mathbf{G}_1, \mathcal{G}_2, \mathcal{G}_3, \mathbf{G}_4$), compare the original tensor $\mathcal{X}$ with the one reconstructed by the TT-SVD.

\end{problem}

\begin{solution} \justifying

\noindent The function is implemented in Julia (v1.4), and is shown in Listing \ref{lst: code1}.  Regarding the present implementation, lines 2--14 define an auxiliary function to compute the tensor train products. The Train Tensor Singular Value Decomposition (TT-SVD) algorithm is then implemented in lines 17--237, following the pseudocode provided in the course material. 

\begin{code}[0.95\textwidth]{The Tensor-Train Singular Value Decomposition (TTSVD) algorithm for Problem 1.}{lst: code1}
# ==== FUNCTIONS ====
# The Tensor-Train product function
function Ã—Â¹(G...)
    # Auxiliary variables with dimension/rank information
    IR  = [size(G[1]); [size(G[i])[2:end] for i=2:length(G)]...]
    I,R = ([IR[i][1] for i=1:length(G)], [IR[i][2] for i=1:length(G)-1])
    
    # Tensor-train product loop
    ğ“§ = zeros(I...)
    for i in CartesianIndices(ğ“§), r in CartesianIndices((R[1],R[2],R[3]))
        ğ“§[i] += G[1][i[1],r[1]]*G[2][r[1],i[2],r[2]]*G[3][r[2],i[3],r[3]]*G[4][r[3],i[4]]
    end
    return ğ“§    # Returns the resulting tensor
end

# The Tensor-Train Singular Value Decomposition (TTSVD) algorithm
function TTSVD(ğ“§)
    I = size(ğ“§)                                     # Auxiliary variable
                                                    #
    XÂ¹ = reshape(ğ“§, (I[1],Î (I[2:4])))               # Unfolds the tensor [ğ“§]â‚ âˆˆ â„(Iâ‚,Iâ‚‚Iâ‚ƒIâ‚„)
    U,Î£,V = svd(XÂ¹);    Î£ = Diagonal(Î£);            # Computes the SVD [ğ“§]â‚ = UÎ£Vá´´
    Râ‚ = sum(diag(Î£) .> 1e-6)                       # Gets the number of nonzero singular-values
    Uâ‚ = U[:,1:Râ‚];  Vâ‚ = (Î£*(V)á´´)[1:Râ‚,:]          # Truncates pair (U,V) given Râ‚
    Gâ‚ = Uâ‚                                         # Stores the 1st factor Gâ‚ âˆˆ â„(Iâ‚,Râ‚)
                                                    #
    XÂ² = reshape(Vâ‚, (Râ‚*I[2], Î (I[3:4])))          # Unfolds the matrix XÂ² = [Vâ‚]â‚ âˆˆ â„(Râ‚Iâ‚‚,Iâ‚ƒIâ‚„)
    U,Î£,V = svd(XÂ²);    Î£ = Diagonal(Î£);            # Computes the SVD XÂ² = UÎ£Vá´´
    Râ‚‚ = sum(diag(Î£) .> 1e-6)                       # Gets the number of nonzero singular-values
    Uâ‚‚ = U[:,1:Râ‚‚];  Vâ‚‚ = (Î£*(V)á´´)[1:Râ‚‚,:]          # Truncates pair (U,V) given Râ‚‚
    ğ“–â‚‚ = reshape(Uâ‚‚, (Râ‚, I[2], Râ‚‚))                # Reshapes the 2nd factor ğ“–â‚‚ âˆˆ â„(Râ‚,Iâ‚‚,Râ‚‚)
                                                    # 
    XÂ³ = reshape(Vâ‚‚, (Râ‚‚*I[3], I[4]))               # Unfolds the matrix XÂ³ = [Vâ‚‚]â‚ âˆˆ â„(Râ‚‚Iâ‚ƒ,Iâ‚„)
    U,Î£,V = svd(XÂ³);    Î£ = Diagonal(Î£);            # Computes the SVD XÂ³ = UÎ£Vá´´
    Râ‚ƒ = sum(diag(Î£) .> 1e-6)                       # Gets the number of nonzero singular-values
    Uâ‚ƒ = U[:,1:Râ‚ƒ];  Vâ‚ƒ = (Î£*(V)á´´)[1:Râ‚ƒ,:]          # Truncates pair (U,V) given Râ‚ƒ
    ğ“–â‚ƒ = reshape(Uâ‚ƒ, (Râ‚‚, I[3], Râ‚ƒ))                # Reshapes the 3rd factor ğ“–â‚ƒ âˆˆ â„(Râ‚‚,Iâ‚ƒ,Râ‚ƒ)
    Gâ‚„ = Vâ‚ƒ[1:Râ‚ƒ,:]                                 # Stores the 4th factor Gâ‚„ âˆˆ â„(Râ‚ƒ,Iâ‚„)

    return (Gâ‚, ğ“–â‚‚, ğ“–â‚ƒ, Gâ‚„)                         # Returns the decomposition factors
end
    
\end{code} 

The function $\times^1(\cdot)$ implements the element-wise TT decomposition through the formula
$$
    x_{i_1,i_2,\cdots,i_N} = \sum_{r_1 = 1}^{R_1} \sum_{r_2 = 1}^{R_2} \sum_{r_3 = 1}^{R_3} \cdots \sum_{r_{N-2} = 1}^{R_{N-2}} \sum_{r_{N-1} = 1}^{R_{N-1}} \mathbf{G}_{1_{i_1,r_1}} \mathcal{G}_{2_{r_1,i_2,r_2}} \mathcal{G}_{3_{r_2,i_3,r_3}} \cdots \mathcal{G}_{{N-1}_{r_{N-2},i_{N-1},r_{N-1}}} \mathbf{G}_{N_{r_N,i_{N-1}}}.
$$
The TT-SVD algorithm follows the procedure described by the tensor network shown in Fig. \ref{fig: f1}

\begin{figure}[ht] \centering
    \includegraphics[width=0.9\textwidth]{imgs/TensorNetwork.pdf}
    \caption{Illustration of the TT-SVD algorithm using a Tensor Network framework.}
    \label{fig: f1}
\end{figure}

The implementation is tested through the script presented at Listing \ref{lst: code2}.
The execution of the assertion tests at lines 10 and 18 do not result in any failed assertion, thus indicating that the function outputs the expected results for the given tests. Specifically, executing lines 11 and 19 shows that, on average, $\| \mathcal{X} - \hat{\mathcal{X}} \|_F^2 \approx 1.2\cdot10^{-26}$ for $\hat{\mathcal{X}} = \hat{\mathbf{G}}_1 \times_2^1 \hat{\mathcal{G}}_2 \times_3^1 \hat{\mathcal{G}}_{3} \times_{4}^1 \hat{\mathbf{G}}_4$ with all TT-cores obtained from the TT-SVD. Thus, it is possible to conclude that factors $(\hat{\mathbf{G}}_1, \hat{\mathcal{G}}_2, \hat{\mathcal{G}}_3, \hat{\mathbf{G}}_4)$ comprise a valid decomposition of the original tensor $\mathcal{X}$. 

\begin{code}[0.9\textwidth]{Script used to test the TTSVD algorithm implementation.}{lst: code2}
# ==== SCRIPT ====
# -- Case 1: Generating ğ“§ from factors (Gâ‚, ğ“–â‚‚, ğ“–â‚ƒ, Gâ‚„) --
# Randomly generates the TT factors and computes the tensor ğ“§ = Gâ‚ Ã—Â¹ ğ“–â‚‚ Ã—Â¹ ğ“–â‚ƒ Ã—Â¹ Gâ‚„
R = [4 3 4];     # Ranks used for the decomposition
Gâ‚ = randn(5,R[1]); ğ“–â‚‚ = randn(R[1],5,R[2]); ğ“–â‚ƒ = randn(R[2],5,R[3]); Gâ‚„ = randn(R[3],5);
ğ“§ = Ã—Â¹(Gâ‚, ğ“–â‚‚, ğ“–â‚ƒ, Gâ‚„);

Gâ‚_, ğ“–â‚‚_, ğ“–â‚ƒ_, Gâ‚„_ = TTSVD(ğ“§);      # Estimates the factors (Gâ‚, ğ“–â‚‚, ğ“–â‚ƒ, Gâ‚„)

@assert ğ“§ â‰ˆ Ã—Â¹(Gâ‚_, ğ“–â‚‚_, ğ“–â‚ƒ_, Gâ‚„_)
@show norm(ğ“§ - Ã—Â¹(Gâ‚_, ğ“–â‚‚_, ğ“–â‚ƒ_, Gâ‚„_))^2

# -- Case 2: Directly generating ğ“§ randomly --
ğ“§ = randn(5, 5, 5, 5);

Gâ‚_, ğ“–â‚‚_, ğ“–â‚ƒ_, Gâ‚„_ = TTSVD(ğ“§);      # Estimates the factors (Gâ‚, ğ“–â‚‚, ğ“–â‚ƒ, Gâ‚„)

@assert ğ“§ â‰ˆ Ã—Â¹(Gâ‚_, ğ“–â‚‚_, ğ“–â‚ƒ_, Gâ‚„_)
@show norm(ğ“§ - Ã—Â¹(Gâ‚_, ğ“–â‚‚_, ğ“–â‚ƒ_, Gâ‚„_))^2

# ==== ====
\end{code} 

Moreover, we verified that the resulting estimated tensors $(\hat{\mathbf{G}}_1, \hat{\mathcal{G}}_2, \hat{\mathcal{G}}_3, \hat{\mathbf{G}}_4)$ are not actually good approximations of the original factors used to generate $\mathcal{X}$. In fact, we have average distances of 
$$
    \| \mathbf{G}_1 - \hat{\mathbf{G}}_1 \|_F^2 = 25.1,\ \ \  
    \| \mathcal{G}_2 - \hat{\mathcal{G}}_2 \|_F^2 = 61.7,\ \ \ 
    \| \mathcal{G}_3 - \hat{\mathcal{G}}_3 \|_F^2 = 62.6,\ \ \ 
    \| \mathbf{G}_4 - \hat{\mathbf{G}}_4 \|_F^2 = 3\cdot 10^4. 
$$
This result is expected, since the decomposition is not unique: is always possible to replace a pair of successive TT-cores $\mathcal{G}_j$ and $\mathcal{G}_{j+1}$ by $\tilde{\mathcal{G}}_j = \mathcal{G}_j \times_{3}^1 \mathbf{M}_j^{-1}$ and $\tilde{\mathcal{G}}_{j+1} = \mathbf{M}_j \times_{2}^1 \mathcal{G}_{j+1}$ without changing the resulting tensor $\mathcal{X}$.

\end{solution}
\vskip0.5cm

%-------------------------------------------------------
\end{document}
